#!/usr/bin/env python3
"""
Scraper pour keur-immo.com - Terrains √† vendre √† Dakar
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import csv
from urllib.parse import urljoin, urlparse
import logging
import os
import boto3
from botocore.exceptions import BotoCoreError, ClientError
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration S3
S3_BUCKET = os.environ.get("S3_BUCKET", "m2dsia-mouhamed-diouf")
S3_KEY_PREFIX = os.environ.get("S3_KEY_PREFIX", "scraping/keur-immo/")


def upload_to_s3(file_path, bucket_name=None, object_name=None):
    """
    T√©l√©verse un fichier vers un bucket S3
    
    Args:
        file_path (str): Chemin local du fichier √† t√©l√©verser
        bucket_name (str, optional): Nom du bucket S3. Par d√©faut: S3_BUCKET
        object_name (str, optional): Nom de l'objet dans S3. Par d√©faut: nom du fichier
    
    Returns:
        bool: True si le t√©l√©versement a r√©ussi, False sinon
    """
    try:
        if bucket_name is None:
            bucket_name = S3_BUCKET
            
        if object_name is None:
            object_name = os.path.basename(file_path)
            
        # S'assurer que le pr√©fixe se termine par un /
        if S3_KEY_PREFIX and not S3_KEY_PREFIX.endswith('/'):
            full_key = f"{S3_KEY_PREFIX}/{object_name}"
        else:
            full_key = f"{S3_KEY_PREFIX}{object_name}"
            
        s3 = boto3.client('s3')
        s3.upload_file(file_path, bucket_name, full_key)
        logger.info(f"Fichier upload√© avec succ√®s vers: s3://{bucket_name}/{full_key}")
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors de l'upload vers S3: {str(e)}")
        return False

class KeurImmoScraper:
    def __init__(self):
        self.base_url = "https://keur-immo.com"
        self.target_url = "https://keur-immo.com/senegal/terrains-a-vendre-dakar/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.properties = []
    
    def get_page(self, url, retries=3):
        """R√©cup√®re une page avec gestion des erreurs et retry"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                logger.warning(f"Tentative {attempt + 1} √©chou√©e pour {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Backoff exponentiel
                else:
                    logger.error(f"Impossible de r√©cup√©rer {url} apr√®s {retries} tentatives")
                    return None
    
    def parse_property_listing(self, soup):
        """Parse une page de listing pour extraire les informations des propri√©t√©s"""
        properties = []
        
        # Adapter ces s√©lecteurs selon la structure HTML r√©elle du site
        property_cards = soup.find_all('div', class_='property-card') or soup.find_all('article')
        
        if not property_cards:
            # Essayer d'autres s√©lecteurs communs
            property_cards = soup.find_all('div', class_=['listing-item', 'property-item', 'item'])
        
        for card in property_cards:
            try:
                property_data = self.extract_property_data(card)
                if property_data:
                    properties.append(property_data)
            except Exception as e:
                logger.warning(f"Erreur lors de l'extraction d'une propri√©t√©: {e}")
                continue
        
        return properties
    
    def extract_property_data(self, card):
        """Extrait les donn√©es d'une propri√©t√© depuis son √©l√©ment HTML"""
        data = {}
        
        # Titre
        title_elem = (card.find(['h1', 'h2', 'h3', 'h4', 'h5']) or 
                     card.find('a', class_=['title', 'property-title', 'listing-title']) or
                     card.find(class_=['title', 'property-title', 'listing-title']))
        data['titre'] = title_elem.get_text(strip=True) if title_elem else 'N/A'
        
        # Prix - recherche plus exhaustive
        price_elem = (card.find(class_=['price', 'prix', 'cost', 'amount', 'property-price']) or
                     card.find('span', string=lambda x: x and any(currency in str(x) for currency in ['FCFA', 'CFA', '‚Ç¨', '$'])) or
                     card.find(string=lambda x: x and any(currency in str(x) for currency in ['FCFA', 'CFA', '‚Ç¨', '$'])))
        if price_elem:
            if hasattr(price_elem, 'get_text'):
                data['prix'] = price_elem.get_text(strip=True)
            else:
                data['prix'] = str(price_elem).strip()
        else:
            data['prix'] = 'N/A'
        
        # Localisation/Adresse
        location_elem = (card.find(class_=['location', 'localisation', 'address', 'lieu', 'zone', 'quartier']) or
                        card.find('i', class_=['fa-map-marker', 'fa-location']) or
                        card.find(string=lambda x: x and any(loc in str(x).lower() for loc in ['dakar', 'pikine', 'gu√©diawaye', 'rufisque'])))
        if location_elem:
            if hasattr(location_elem, 'get_text'):
                data['localisation'] = location_elem.get_text(strip=True)
            elif hasattr(location_elem, 'parent'):
                data['localisation'] = location_elem.parent.get_text(strip=True)
            else:
                data['localisation'] = str(location_elem).strip()
        else:
            data['localisation'] = 'N/A'
        
        # Surface - recherche plus compl√®te
        surface_patterns = ['m¬≤', 'hectare', 'ha', 'are', 'superficie']
        surface_elem = None
        for pattern in surface_patterns:
            surface_elem = card.find(string=lambda x: x and pattern in str(x).lower())
            if surface_elem:
                break
        
        if not surface_elem:
            surface_elem = card.find(class_=['surface', 'area', 'size', 'superficie'])
        
        if surface_elem:
            if hasattr(surface_elem, 'get_text'):
                data['surface'] = surface_elem.get_text(strip=True)
            else:
                data['surface'] = str(surface_elem).strip()
        else:
            data['surface'] = 'N/A'
        
        # Lien vers la page d√©taill√©e
        link_elem = card.find('a', href=True)
        if link_elem:
            data['lien'] = urljoin(self.base_url, link_elem['href'])
        else:
            data['lien'] = 'N/A'
        
        # Description courte
        desc_elem = (card.find(class_=['description', 'excerpt', 'summary', 'content']) or
                    card.find('p'))
        data['description'] = desc_elem.get_text(strip=True) if desc_elem else 'N/A'
        
        # ID de la propri√©t√©
        id_elem = card.get('id') or card.get('data-id')
        data['id_propriete'] = id_elem if id_elem else 'N/A'
        
        # Type de propri√©t√©
        type_elem = card.find(class_=['type', 'category', 'property-type'])
        data['type'] = type_elem.get_text(strip=True) if type_elem else 'Terrain'
        
        # Date de publication
        date_elem = (card.find(class_=['date', 'published', 'created']) or
                    card.find('time'))
        if date_elem:
            data['date_publication'] = date_elem.get_text(strip=True) or date_elem.get('datetime', 'N/A')
        else:
            data['date_publication'] = 'N/A'
        
        # Agent/Contact
        agent_elem = (card.find(class_=['agent', 'contact', 'seller', 'owner']) or
                     card.find(string=lambda x: x and 'agent' in str(x).lower()))
        if agent_elem:
            if hasattr(agent_elem, 'get_text'):
                data['agent'] = agent_elem.get_text(strip=True)
            else:
                data['agent'] = str(agent_elem).strip()
        else:
            data['agent'] = 'N/A'
        
        # T√©l√©phone
        phone_elem = (card.find(class_=['phone', 'tel', 'telephone']) or
                     card.find('a', href=lambda x: x and x.startswith('tel:')) or
                     card.find(string=lambda x: x and any(char.isdigit() for char in str(x)) and len([c for c in str(x) if c.isdigit()]) >= 8))
        if phone_elem:
            if hasattr(phone_elem, 'get_text'):
                data['telephone'] = phone_elem.get_text(strip=True)
            elif hasattr(phone_elem, 'get'):
                data['telephone'] = phone_elem.get('href', '').replace('tel:', '')
            else:
                data['telephone'] = str(phone_elem).strip()
        else:
            data['telephone'] = 'N/A'
        
        # Images
        img_elems = card.find_all('img')
        images = []
        for img in img_elems:
            src = img.get('src') or img.get('data-src')
            if src:
                images.append(urljoin(self.base_url, src))
        data['images'] = images if images else []
        data['nombre_images'] = len(images)
        
        # Caract√©ristiques suppl√©mentaires
        features = []
        feature_keywords = ['cl√¥tur√©', 'titre foncier', 'viabilis√©', '√©lectricit√©', 'eau', '√©gout', 'bitum√©']
        for keyword in feature_keywords:
            if card.find(string=lambda x: x and keyword.lower() in str(x).lower()):
                features.append(keyword)
        data['caracteristiques'] = features
        
        # Statut (√† vendre, vendu, etc.)
        status_elem = card.find(class_=['status', 'statut', 'badge'])
        data['statut'] = status_elem.get_text(strip=True) if status_elem else '√Ä vendre'
        
        return data if data['titre'] != 'N/A' else None
    
    def get_detailed_property_info(self, property_url):
        """R√©cup√®re les d√©tails complets d'une propri√©t√© depuis sa page d√©di√©e"""
        if property_url == 'N/A':
            return {}
        
        response = self.get_page(property_url)
        if not response:
            return {}
        
        soup = BeautifulSoup(response.content, 'html.parser')
        details = {}
        
        # Description compl√®te
        desc_elem = (soup.find(class_=['description', 'content', 'property-description', 'details']) or
                    soup.find('div', string=lambda x: x and len(str(x)) > 100))
        if desc_elem:
            details['description_complete'] = desc_elem.get_text(strip=True)
        
        # Prix d√©taill√©
        price_elem = soup.find(class_=['price', 'prix', 'cost', 'property-price'])
        if price_elem:
            details['prix_detaille'] = price_elem.get_text(strip=True)
        
        # Caract√©ristiques d√©taill√©es
        characteristics = {}
        
        # Recherche dans les listes de caract√©ristiques
        feature_lists = soup.find_all(['ul', 'dl', 'div'], class_=['features', 'characteristics', 'details', 'specs'])
        for feature_list in feature_lists:
            items = feature_list.find_all(['li', 'dt', 'dd', 'span', 'div'])
            for item in items:
                text = item.get_text(strip=True)
                if ':' in text:
                    key, value = text.split(':', 1)
                    characteristics[key.strip()] = value.strip()
                elif any(keyword in text.lower() for keyword in ['m¬≤', 'hectare', 'superficie', 'surface']):
                    characteristics['surface_detaillee'] = text
                elif any(keyword in text.lower() for keyword in ['prix', 'co√ªt', 'montant']):
                    characteristics['prix_info'] = text
        
        details['caracteristiques_detaillees'] = characteristics
        
        # Coordonn√©es GPS si disponibles
        map_elem = soup.find(['iframe', 'div'], src=lambda x: x and 'maps' in str(x)) or soup.find(attrs={'data-lat': True})
        if map_elem:
            lat = map_elem.get('data-lat') or 'N/A'
            lng = map_elem.get('data-lng') or 'N/A'
            details['coordonnees'] = {'latitude': lat, 'longitude': lng}
        
        # Galerie d'images compl√®te
        img_gallery = soup.find(class_=['gallery', 'images', 'photos'])
        if img_gallery:
            images = []
            for img in img_gallery.find_all('img'):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    images.append(urljoin(self.base_url, src))
            details['galerie_images'] = images
        
        # Informations de contact d√©taill√©es
        contact_section = soup.find(class_=['contact', 'agent-info', 'seller-info'])
        if contact_section:
            contact_info = {}
            
            # Nom de l'agent
            name_elem = contact_section.find(['h3', 'h4', 'span'], class_=['name', 'agent-name'])
            if name_elem:
                contact_info['nom_agent'] = name_elem.get_text(strip=True)
            
            # T√©l√©phone
            phone_elem = contact_section.find('a', href=lambda x: x and x.startswith('tel:'))
            if phone_elem:
                contact_info['telephone_agent'] = phone_elem.get('href').replace('tel:', '')
            
            # Email
            email_elem = contact_section.find('a', href=lambda x: x and x.startswith('mailto:'))
            if email_elem:
                contact_info['email_agent'] = email_elem.get('href').replace('mailto:', '')
            
            details['contact_detaille'] = contact_info
        
        # Propri√©t√©s similaires ou recommand√©es
        similar_section = soup.find(class_=['similar', 'related', 'recommended'])
        if similar_section:
            similar_properties = []
            for prop in similar_section.find_all('a', href=True):
                similar_properties.append({
                    'titre': prop.get_text(strip=True),
                    'lien': urljoin(self.base_url, prop['href'])
                })
            details['proprietes_similaires'] = similar_properties[:5]  # Limiter √† 5
        
        # Informations l√©gales
        legal_info = {}
        legal_keywords = ['titre foncier', 'cadastre', 'permis', 'autorisation', 'zone']
        for keyword in legal_keywords:
            elem = soup.find(string=lambda x: x and keyword.lower() in str(x).lower())
            if elem:
                legal_info[keyword] = elem.strip()
        
        if legal_info:
            details['informations_legales'] = legal_info
        
        # Date de derni√®re mise √† jour
        update_elem = soup.find(class_=['updated', 'modified', 'last-update'])
        if update_elem:
            details['derniere_mise_a_jour'] = update_elem.get_text(strip=True)
        
        return details
    
    def get_total_pages(self, soup):
        """D√©termine le nombre total de pages"""
        pagination = soup.find(class_=['pagination', 'pager'])
        if pagination:
            page_links = pagination.find_all('a')
            if page_links:
                try:
                    return max([int(link.get_text()) for link in page_links if link.get_text().isdigit()])
                except ValueError:
                    pass
        return 1
    
    def scrape_all_pages(self, get_details=True):
        """Scrape toutes les pages de r√©sultats avec option pour les d√©tails complets"""
        logger.info(f"D√©but du scraping de {self.target_url}")
        
        # Premi√®re page pour d√©terminer le nombre total
        response = self.get_page(self.target_url)
        if not response:
            logger.error("Impossible de r√©cup√©rer la premi√®re page")
            return
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extraire les propri√©t√©s de la premi√®re page
        properties = self.parse_property_listing(soup)
        self.properties.extend(properties)
        logger.info(f"Page 1: {len(properties)} propri√©t√©s trouv√©es")
        
        # D√©terminer le nombre total de pages
        total_pages = self.get_total_pages(soup)
        logger.info(f"Nombre total de pages d√©tect√©: {total_pages}")
        
        # Scraper les pages suivantes
        for page_num in range(2, total_pages + 1):
            page_url = f"{self.target_url}?page={page_num}"
            logger.info(f"Scraping page {page_num}/{total_pages}")
            
            response = self.get_page(page_url)
            if response:
                soup = BeautifulSoup(response.content, 'html.parser')
                properties = self.parse_property_listing(soup)
                self.properties.extend(properties)
                logger.info(f"Page {page_num}: {len(properties)} propri√©t√©s trouv√©es")
            
            # Pause entre les requ√™tes pour √™tre respectueux
            time.sleep(1)
        
        logger.info(f"Scraping des listes termin√©. Total: {len(self.properties)} propri√©t√©s")
        
        # R√©cup√©rer les d√©tails complets si demand√©
        if get_details and self.properties:
            logger.info("R√©cup√©ration des d√©tails complets pour chaque propri√©t√©...")
            
            for i, property_data in enumerate(self.properties):
                logger.info(f"D√©tails {i+1}/{len(self.properties)}: {property_data.get('titre', 'N/A')}")
                
                # R√©cup√©rer les d√©tails complets
                detailed_info = self.get_detailed_property_info(property_data.get('lien', 'N/A'))
                
                # Fusionner les d√©tails avec les donn√©es existantes
                property_data.update(detailed_info)
                
                # Pause entre les requ√™tes d√©taill√©es
                time.sleep(2)
            
            logger.info("R√©cup√©ration des d√©tails termin√©e")
        
        logger.info(f"Scraping complet termin√©. Total: {len(self.properties)} propri√©t√©s avec d√©tails")
    
    def save_to_json(self, filename='keur_immo_terrains.json'):
        """Sauvegarde les donn√©es en JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.properties, f, ensure_ascii=False, indent=2)
        logger.info(f"Donn√©es sauvegard√©es dans {filename}")
    
    def save_to_csv(self, filename='keur_immo_terrains.csv'):
        """Sauvegarde les donn√©es en CSV"""
        if not self.properties:
            logger.warning("Aucune donn√©e √† sauvegarder")
            return
        
        fieldnames = self.properties[0].keys()
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.properties)
        logger.info(f"Donn√©es sauvegard√©es dans {filename}")
    
    def analyze_data(self):
        """Analyse les donn√©es r√©cup√©r√©es et affiche des statistiques"""
        if not self.properties:
            print("Aucune donn√©e √† analyser")
            return
        
        print(f"\n=== ANALYSE DES DONN√âES ===")
        print(f"Nombre total de propri√©t√©s: {len(self.properties)}")
        
        # Analyse des prix
        prices = []
        for prop in self.properties:
            prix_text = prop.get('prix', 'N/A')
            if prix_text != 'N/A':
                # Extraire les chiffres du prix
                import re
                numbers = re.findall(r'[\d\s]+', prix_text.replace(',', '').replace('.', ''))
                if numbers:
                    try:
                        price = int(''.join(numbers[0].split()))
                        prices.append(price)
                    except ValueError:
                        pass
        
        if prices:
            print(f"\n--- Analyse des prix ---")
            print(f"Prix minimum: {min(prices):,} FCFA")
            print(f"Prix maximum: {max(prices):,} FCFA")
            print(f"Prix moyen: {sum(prices)//len(prices):,} FCFA")
        
        # Analyse des localisations
        locations = {}
        for prop in self.properties:
            loc = prop.get('localisation', 'N/A')
            if loc != 'N/A':
                locations[loc] = locations.get(loc, 0) + 1
        
        if locations:
            print(f"\n--- Top 10 des localisations ---")
            sorted_locations = sorted(locations.items(), key=lambda x: x[1], reverse=True)
            for loc, count in sorted_locations[:10]:
                print(f"{loc}: {count} propri√©t√©s")
        
        # Analyse des surfaces
        surfaces = []
        for prop in self.properties:
            surface_text = prop.get('surface', 'N/A')
            if surface_text != 'N/A':
                import re
                numbers = re.findall(r'\d+', surface_text)
                if numbers:
                    try:
                        surface = int(numbers[0])
                        surfaces.append(surface)
                    except ValueError:
                        pass
        
        if surfaces:
            print(f"\n--- Analyse des surfaces ---")
            print(f"Surface minimum: {min(surfaces)} m¬≤")
            print(f"Surface maximum: {max(surfaces)} m¬≤")
            print(f"Surface moyenne: {sum(surfaces)//len(surfaces)} m¬≤")
        
        # Analyse des caract√©ristiques
        all_features = []
        for prop in self.properties:
            features = prop.get('caracteristiques', [])
            all_features.extend(features)
        
        if all_features:
            feature_counts = {}
            for feature in all_features:
                feature_counts[feature] = feature_counts.get(feature, 0) + 1
            
            print(f"\n--- Caract√©ristiques les plus communes ---")
            sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)
            for feature, count in sorted_features[:5]:
                print(f"{feature}: {count} propri√©t√©s")
        
        # Propri√©t√©s avec le plus d'images
        with_images = [prop for prop in self.properties if prop.get('nombre_images', 0) > 0]
        if with_images:
            print(f"\n--- Images ---")
            print(f"Propri√©t√©s avec images: {len(with_images)}")
            avg_images = sum(prop.get('nombre_images', 0) for prop in with_images) / len(with_images)
            print(f"Nombre moyen d'images: {avg_images:.1f}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Scraper pour keur-immo.com')
    parser.add_argument('--no-details', action='store_true', 
                       help='Ne pas r√©cup√©rer les d√©tails complets (plus rapide)')
    parser.add_argument('--max-properties', type=int, default=None,
                       help='Nombre maximum de propri√©t√©s √† scraper')
    
    args = parser.parse_args()
    
    scraper = KeurImmoScraper()
    
    # Scraper avec ou sans d√©tails complets
    get_details = not args.no_details
    scraper.scrape_all_pages(get_details=get_details)
    
    # Limiter le nombre de propri√©t√©s si sp√©cifi√©
    if args.max_properties and len(scraper.properties) > args.max_properties:
        scraper.properties = scraper.properties[:args.max_properties]
        logger.info(f"Limitation √† {args.max_properties} propri√©t√©s")
    
    if scraper.properties:
        # Sauvegarder les donn√©es
        scraper.save_to_json()
        scraper.save_to_csv()
        
        # Analyser les donn√©es
        scraper.analyze_data()
        
        # Afficher quelques exemples d√©taill√©s
        print(f"\n=== EXEMPLES D√âTAILL√âS ===")
        for i, prop in enumerate(scraper.properties[:2]):
            print(f"\n--- Propri√©t√© {i+1} ---")
            for key, value in prop.items():
                if isinstance(value, list) and len(value) > 3:
                    print(f"  {key}: {len(value)} √©l√©ments - {value[:3]}...")
                elif isinstance(value, dict):
                    print(f"  {key}: {len(value)} d√©tails")
                    for sub_key, sub_value in list(value.items())[:3]:
                        print(f"    {sub_key}: {sub_value}")
                else:
                    print(f"  {key}: {value}")
        
        print(f"\n=== FICHIERS G√âN√âR√âS ===")
        print(f"üìÑ keur_immo_terrains.json - Donn√©es compl√®tes en JSON")
        print(f"üìä keur_immo_terrains.csv - Donn√©es tabulaires en CSV")
        print(f"üìà Total: {len(scraper.properties)} propri√©t√©s avec tous les d√©tails")
        
    else:
        print("‚ùå Aucune donn√©e r√©cup√©r√©e. V√©rifiez la structure du site.")
        print("üí° Essayez d'abord: python test_scraper.py")

if __name__ == "__main__":
    main()